model:
  vocab_size: 50257
  d_model: 768
  n_layers: 24
  n_heads: 12
  n_kv_heads: 4
  d_ff_multiplier: 2.667
  max_seq_len: 1024
  dropout: 0.0
  rope_theta: 10000.0

training:
  learning_rate: 6.0e-4
  min_lr: 6.0e-5
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  batch_size: 16
  gradient_accumulation_steps: 8
  max_steps: 50000
  warmup_steps: 1000
  log_interval: 10
  eval_interval: 500
  save_interval: 5000
  eval_steps: 50
  dataset_name: "roneneldan/TinyStories"
  seq_len: 1024
  dtype: "bfloat16"
  compile: true
  output_dir: "checkpoints/medium"
  wandb_project: "transformer-circuits-lab"
  wandb_run_name: "medium-125M"
  use_wandb: true
